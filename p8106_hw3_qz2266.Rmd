---
title: "p8106_hw3_qz2266"
author: "Qing Zhou"
date: "2023-03-23"
output: pdf_document
---


```{r setup, include=FALSE, message=FALSE, warning=FALSE}

library(caret)
library(MASS)
library(pROC)
library(klaR)
library(glmnet)
library(vip)
library(tidyverse)
library(knitr)


knitr::opts_chunk$set(warning = FALSE)
```


## Data import

In this problem, we will develop a model to predict whether a given car gets 
high or low gas mileage based on the dataset “auto.csv”. The dataset contains 
392 observations. The response variable is mpg cat, which indicates whether the 
miles per gallon of a car is high or low. 

```{r read}
auto = read.csv("data/auto.csv") %>% 
  mutate(
    mpg_cat = as.factor(mpg_cat),
    mpg_cat = fct_relevel(mpg_cat, c("low", "high")),
    year = factor(year),
    origin = as.factor(origin)) 
```

Note: Here I mutated `year` as a factor variable, since I don't assume there's a linear relationship between model year and gas millage. 


## Data spliting

Split the dataset into two parts: training data (70%) and test data (30%):

```{r split}
set.seed(1)

# partition
trainRows <- createDataPartition(y = auto$mpg_cat, p = 0.7,list = FALSE)
train_data = auto[trainRows, ]
test_data = auto[-trainRows, ]

x = model.matrix(mpg_cat ~ ., train_data)[,-1]
y = train_data$mpg_cat
x_test = model.matrix(mpg_cat ~ ., test_data)[,-1]
y_test = test_data$mpg_cat
```


## (a) Logistic regression model

1. Perform a logistic regression using the training data:

```{r}
set.seed(1)
contrasts(auto$mpg_cat)

# model fitting
logit_fit <- glm(mpg_cat ~ .,
               data = auto,
               subset = trainRows,
               family = binomial(link = "logit"))
summary(logit_fit)
```

Predictors in the logistic model that are statistically significant at the 5% 
level of significance are listed as below: 
- `weight` (vehicle weight (lbs.))
- `year79` (model year 79)
- `year80` (model year 80)
- `year81` (model year 81) 
- `origin2` (European origin)
- `origin3` (Japanese origin).


2. Set the probability threshold to 0.5 to determine class labels and compute 
the confusion matrix using the test data:

```{r}
# forecast new observations in the testing set
test.pred.prob <- predict(logit_fit, newdata = test_data,
                          type = "response")
test.pred <- rep("low", length(test.pred.prob))
test.pred[test.pred.prob > 0.5] = "high"

#confusion matrix
logit_cm = confusionMatrix(data = factor(test.pred, levels = c("low", "high")),
                                         reference = y_test,
                                         positive = "high")
logit_cm

# extract overall accuracy of the model
logit_cm$byClass["Balanced Accuracy"]
```

- The confusion matrix shows the number of correct and incorrect predictions per class. It helps in understanding the classes that are being confused by model as other class. The rows refer to predicted class, while the columns indicate the actual class. Therefore,  the number of true lows, true highs, false lows, and false highs are 50, 54, 4, and 8, respectively. Here true lows means the model has 50 correct predictions as having low gas mileage, and true high means the model has 54 correct predictions as having high gas mileage. False high indicate there are 8 count of number of low gas mileage that were misclassified as high gas mileage, and false low indicate there are 4 count of number of high gas mileage that were misclassified as low gas mileage.

- The overall prediction accuracy could be calculated as $\frac{50+54}{50+8+4+54} = 0.897$, with 95% CI (0.8263, 0.9454). Balance accuracy is also 0.897. The No Information Rate (NIR) is 0.5. The no information rate is 0.5,  which means if we made the same class prediction for all observations given no information, our model would be 50% accurate, which is not very ideal. P-value is < 2e-16. So the accuracy is significantly different from no information rate.

- The kappa coefficient is 0.7931, which measures the agreement between classification and truth values. A kappa value of 1 represents perfect agreement, while a value of 0 represents no agreement. So here the kappa value indicates substantial agreement.

- Sensitivity(the percentage of true positives among the positive observations)is 0.931 while specificity(the percentage of true negatives among the negative observations) is 0.8621. Both are high. Positive Predictive Value (PPV) measures the ratio of true positive predictions considering all positive predictions. Negative Predictive Value (NPV) measures the ratio of true negative predictions considering all negative predictions.Here PPV is 0.871 while NPV is 0.9259.


3. We can also fit a logistic regression using caret.Similarly, this model shows the same 6 predictors to be statistically significant, and they are `weight`, `year79`,  `year80`,  `year81`, `origin2`, `origin3`:

```{r}
# logistic model using caret
set.seed(1)
ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

logit_caret = train(x = auto[trainRows, 1:7],
                  y = auto$mpg_cat[trainRows],
                  method = "glm",
                  metric = "ROC",
                  trControl = ctrl)
summary(logit_caret)
```


## (b). MARS model

Train a multivariate adaptive regression spline (MARS) model using the training
data:

```{r warning=F, message=F}
set.seed(1)

mars_fit <- train(x = auto[trainRows,1:7],
                    y = auto$mpg_cat[trainRows],
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:4,
                                           nprune = 2:20),
                    metric = "ROC",
                    trControl = ctrl)

summary(mars_fit)
plot(mars_fit)


# extract the optimal tuning parameters
kable(mars_fit$bestTune, "simple")
coef(mars_fit$finalModel) 
vip(mars_fit$finalModel)
```

- From the first plot,  we can see that when degree = 1 with 14 terms, the curve reaches its highest point. We got the same result from the extracted best tunes from the final MARS model.

- From the variable importance measurement plot (VIP), we found 8 variables of `displacement`, `cylinders`, `weight`, `year79-82` and `horsepower` contributes most to the model. Among them, `displacement`, `cylinders`, and `weight` are the most important. `year71`and `acceleration` has zero importance. 

- The final MARS model has RSS = 11.873, R-squared = 0.8279, which is pretty high.


## (c). LDA

1. Perform LDA using the training data:


```{r}
## LDA fit
set.seed(1)
lda.fit = lda(mpg_cat~., data = auto,
               subset = trainRows)

lda.fit$scaling
head(predict(lda.fit)$x)
mean(predict(lda.fit)$x) ## seen as 0, means data is centered
```

2. Plot the linear discriminants in LDA

```{r fig.width=10, fig.asp=0.8}
plot(lda.fit)
```

The linear discriminate is plotted above within two classes, and we have k = 2-1 = 1 linear discriminants.The plot shows there are a little overlap observed between two groups but not very severe. 


3. Using caret to fit LDA model shows the same predictors and coefficients:

```{r}
set.seed(1)
lda_caret = train(mpg_cat ~ .,
                  data = train_data,
                  method = "lda",
                  metric = "ROC",
                  trControl = ctrl)

lda_caret$bestTune
coef(lda_caret$finalModel)
```

## (d). Model selection

Which model will you use to predict the response variable? Plot its ROC curve using
the test data. Report the AUC and the misclassification error rate.

1. Compare 3 models:

```{r}
res <- resamples(list(LOGIT = logit_caret,
                      MARS = mars_fit,
                      LDA = lda_caret))
summary(res)

bwplot(res, metric = "ROC")
```

- From the summary and the box-plot, we compared the mean and median and overall distribution of AUC of three models and concluded LDA has the largest AUC.

- Note: AUC is referred to as "ROC" in R output.


2. Now let’s look at the test set performance:

```{r}
set.seed(1)

logit_pred_prob <- predict(logit_caret, newdata = auto[-trainRows,], type = "prob")[,2]
mars_pred_prob <- predict(mars_fit, newdata = auto[-trainRows,], type = "prob")[,2]
lda_pred_prob <- predict(lda_caret, newdata = auto[-trainRows,], type = "prob")[,2]

roc.logit <- roc(auto$mpg_cat[-trainRows], logit_pred_prob)
roc.mars <- roc(auto$mpg_cat[-trainRows], mars_pred_prob)
roc.lda <- roc(auto$mpg_cat[-trainRows], lda_pred_prob)

# AUC
auc <- c(roc.logit$auc[1], roc.mars$auc[1], roc.lda$auc[1])
auc

# plot the ROC curve using the test data
plot(roc.logit, legacy.axes = TRUE)
plot(roc.mars, col = 2, add = TRUE)
plot(roc.lda, col = 3, add = TRUE)
modelNames <- c("logit", "mars", "lda")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
col = 1:3, lwd = 2)
```

- The value for area under the curve (AUC) suggests the model performance. From the results above we know, AUC of Logit, MARS and LDA model are 0.9619501, 0.9368312, and 0.9622473 respectively. Therefore, LDA with the largest AUC is the most favorable. 

- In conclusion, LDA model has the best performance and we choose **LDA model** to predict the response variable.



3. Misclassification error rate:

```{r, message=F}

# confusion matrix to get the overall accuracy of the models

logit_pred = rep("low", length(logit_pred_prob))
logit_pred[logit_pred_prob > 0.5] = "high"
logit_cm = confusionMatrix(data = factor(logit_pred, levels = c("low", "high")),
                                       reference = y_test,
                                       positive = "high")

mars_pred = rep("low", length(mars_pred_prob))
mars_pred[mars_pred_prob > 0.5] = "high"
mars_cm = confusionMatrix(data = factor(mars_pred, levels = c("low", "high")),
                                        reference = y_test,
                                        positive = "high")


lda_pred = rep("low", length(lda_pred_prob))
lda_pred[lda_pred_prob > 0.5] = "high"
lda_cm = confusionMatrix(data = factor(lda_pred, levels = c("low", "high")),
                                       reference = y_test,
                                       positive = "high")


misclas_table = matrix(c(modelNames,
                         (1 - logit_cm$byClass[["Balanced Accuracy"]]), 
                         (1 - mars_cm$byClass[["Balanced Accuracy"]]), 
                         (1 - lda_cm$byClass[["Balanced Accuracy"]])))


kable(misclas_table[,], "simple")

```

The misclassification error rate is a metric that tells us the percentage of observations that were incorrectly predicted by some classification model. The value for misclassification rate can range from 0 to 1 where: 0 represents a model that had zero incorrect predictions. It is defined as 1 - accuracy, or 1 - the overall fraction of correct predictions. The misclassification error rate of Logit, MARS and LDA model is 0.1034, 0.1207 and 0.1034 respectively. Thus, Logit and LDA is better than MARS.

